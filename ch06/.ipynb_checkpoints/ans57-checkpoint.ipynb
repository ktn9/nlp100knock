{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_table('train.txt', header=None)\n",
    "valid = pd.read_table('valid.txt', header=None)\n",
    "test = pd.read_table('test.txt', header=None)\n",
    "\n",
    "cols = ['CATEGORY', 'TITLE']\n",
    "train.columns = cols\n",
    "valid.columns = cols\n",
    "test.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as PS\n",
    "import re\n",
    "ps = PS() #steming用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#出現頻度の高い単語の除去\n",
    "def remove_stopwords(text):\n",
    "    #入力と出力は同じstring形式\n",
    "    stop_words = set(stopwords.words(\"english\")) #stopwords\n",
    "    \n",
    "    #文区切りと単語区切りを順に行い　単語をストップワードと照らし合わせる\n",
    "    period=[]\n",
    "    for i in re.split(\"[.]\",text):\n",
    "        word=[j for j in i.split(\" \") if j != \"\" and j not in stop_words]\n",
    "        period.append(word)\n",
    "        word=[]\n",
    "    \n",
    "    #元の形に復元\n",
    "    tmp=[]\n",
    "    for k in period:\n",
    "        if k != []:\n",
    "            tmp.append(' '.join(k))\n",
    "\n",
    "    result='.'.join(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    replaced_text = text.lower()\n",
    "    replaced_text = re.sub(r'[【】]', '', replaced_text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', '', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', '', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[{}\\{\\}]', '', replaced_text)   #  {}\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n",
    "    replaced_text = re.sub(r'\\-',' ', replaced_text) #ハイフンは空白へ\n",
    "    replaced_text = re.sub(r'　', ' ', replaced_text) # 全角空白の除去\n",
    "    replaced_text = re.sub(r'  ', '', replaced_text) #２連続の半角空白を1つに\n",
    "    replaced_text = re.sub(r'\\d+\\.\\d+','', replaced_text) #小数点を含む数列の除去\n",
    "    replaced_text = re.sub(r'\\;','', replaced_text) #セミコロン\n",
    "    replaced_text = re.sub(r'\\:','', replaced_text) #コロン\n",
    "    replaced_text = re.sub(r'\\'','', replaced_text) #クオーテーション\n",
    "    replaced_text = re.sub(r'\\`','', replaced_text) #クオーテーション\n",
    "    replaced_text = re.sub(r'\\,','', replaced_text) #カンマ\n",
    "    replaced_text = re.sub(r'\\_','', replaced_text) #アンダーバー\n",
    "    replaced_text = re.sub(r'\\\\','', replaced_text) #バックスラッシュ\n",
    "    replaced_text = re.sub(r'\\?','', replaced_text) #クエスチョン\n",
    "    replaced_text = re.sub(r'\\!','', replaced_text) #感嘆符　（ピリオドど同様に文末に使われる）\n",
    "    replaced_text = re.sub(r'\\+','', replaced_text) #プラス\n",
    "    replaced_text = re.sub(r'\\*','', replaced_text) #アスタリスク\n",
    "    replaced_text = re.sub(r'\\/','', replaced_text) #スラッシュ\n",
    "    replaced_text = re.sub(r'\\<','', replaced_text) #小なり\n",
    "    replaced_text = re.sub(r'\\>','', replaced_text) #大なり\n",
    "    replaced_text = re.sub(r'\\=','', replaced_text) #イコール\n",
    "    replaced_text = re.sub(r'\\%','', replaced_text) #パーセント\n",
    "    replaced_text = re.sub(r'\\&','', replaced_text) #アンパサント\n",
    "    replaced_text = re.sub(r'\\$','', replaced_text) #ドル\n",
    "    replaced_text = re.sub(r'\\#','', replaced_text) #シャープ\n",
    "    replaced_text = re.sub(r'.\\..\\.','', replaced_text) #U.S. or U.N.の除去\n",
    "    #replaced_text = re.sub(r'\\d+','', replaced_text) #数列の除去\n",
    "    replaced_text = re.sub(r'\\d{1,3}','', replaced_text) #1から3桁の数列の除去\n",
    "    replaced_text = re.sub(r'\\d{5,}','', replaced_text) #5桁以上の数列の除去\n",
    "    replaced_text = ps.stem(\"%s\" % replaced_text) #語幹抽出\n",
    "    return remove_stopwords(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"shaped_txt\"] = list(map(clean_text,train[\"TITLE\"]))\n",
    "valid[\"shaped_txt\"] = list(map(clean_text,valid[\"TITLE\"]))\n",
    "test[\"shaped_txt\"] = list(map(clean_text,test[\"TITLE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"type\"] = \"train\"\n",
    "valid[\"type\"] = \"valid\"\n",
    "test[\"type\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, valid, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_vec = vectorizer.fit_transform(data[\"shaped_txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, pd.DataFrame(txt_vec.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.query('type==\"train\"').drop(cols + ['type','shaped_txt'], axis=1)\n",
    "valid = data.query('type==\"valid\"').drop(cols + ['type','shaped_txt'], axis=1)\n",
    "test = data.query('type==\"test\"').drop(cols + ['type','shaped_txt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kitano\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='sag')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_train = train\n",
    "\n",
    "y_train = pd.read_table('train.txt', header=None)\n",
    "\n",
    "y_train[0]\n",
    "le = LabelEncoder()\n",
    "le = le.fit(y_train[0])\n",
    "y_train = le.transform(y_train[0])\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', solver='sag', random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14022"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video', 'activision', 'ebola', 'aereo', 'fcc', 'nintendo', 'samsung', 'microsoft', 'sony', 'sprint']\n",
      "['bank', 'fed', 'ecb', 'stocks', 'china', 'euro', 'dollar', 'growth', 'fitch', 'ukraine']\n",
      "['google', 'china', 'facebook', 'gm', 'billion', 'scientists', 'buy', 'ceo', 'sales', 'data']\n",
      "['kardashian', 'chris', 'paul', 'miley', 'cyrus', 'film', 'kim', 'transformers', 'george', 'spotify']\n",
      "['ceo', 'gm', 'climate', 'twitter', 'apple', 'profit', 'costs', 'deal', 'bank', 'amazon']\n",
      "['ebola', 'drug', 'fda', 'cancer', 'mers', 'virus', 'cigarettes', 'cdc', 'study', 'alzheimers']\n",
      "['stocks', 'grows', 'drug', 'american', 'shares', 'raise', 'spotify', 'wants', 'york', 'second']\n",
      "['google', 'microsoft', 'apple', 'facebook', 'fcc', 'nasa', 'comcast', 'tesla', 'gm', 'climate']\n"
     ]
    }
   ],
   "source": [
    "for i in clf.coef_:\n",
    "    ind_ = [(m,l) for l,m in enumerate(i)]\n",
    "    top_lst = [features[int(j[1])] for j in sorted(ind_)[:10]]\n",
    "    print(top_lst)\n",
    "    bottom_lst = [features[int(k[1])] for k in sorted(ind_,reverse=True)[:10]]\n",
    "    print(bottom_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
